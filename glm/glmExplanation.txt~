
# Explaining GLMs 

## Likelihood

Time to try and explain the maths behind GLM on twitter. The plan: Likelihood -> Linear regression -> GLM.

Likelihood is similar to probability. If we know the model we talk about probability of data. P(heads) = 0.5 on a fair coin. #glm

If we know the data we talk about likelihood of parameters. If we see 5 heads, what's the likelihood that the coin is fair i.e. 50/50? #glm



In a continuous world we talk about probability density which doesn't relate to our typical understanding of probability. #glm

The probability of getting exactly 0.1 from a random number between 0 and 1 is zero, so we talk about density instead. #glm



Here is the normal probability density function with mean μ and sd σ. For any x we get a probability density. #glm

In this case where we know μ=0 the probability density of drawing an x value of -1 is about 0.25. #glm



Now imagine we don't know the value of the mean μ but we have seen a datapoint with x=-1. #glm

The likelihood that the datapoint came from a distribution with μ=0 is 0.25. Reverse logic to probability. #glm



We can calculate the likelihood for each value of μ given that we've seen our one data point at x=-1. 

This is the *drumroll* likelihood function. The Maximum likelihood estimate of μ is at the highest value (μ=-1 here). 	



If we see two data points, to find the probability that they're both froma norm with μ=0 we mulitiply the probs. 

@bulboussquidge  IMPORTANT: Note assumption is that the two data points are independent, and from the same distribution.

We multiply them because thats how you get P(event A AND event B). Probability of heads AND heads is 0.5 x 0.5 = 0.25

If we see n data points, we multiply them all together (written as Π). And this is our likelihood function. #glm 

@bulboussquidge Also note integrating L(mu|x) with respect to mu does not result in the value 1 - hence likelihoods are not density functions



## Max Likelihood LM

Yesterday the example I mostly used was estimating the mean μ of a normal distribution. Today we move onto the linear model. #glm


So the equation of an lm. 
y = βx + c + ε
ε ~ Norm(0, σ^2)
#glm

We assume our errors are centered on our line so we know μ=0. Instead, we want to estimate the unknown β. #glm



As before, I find it easier to think about probability of a datapoint, assuming a parameter, then do conceptual flip to likelihood. #glm


For a linear model where we say we know β, the probability of a datapoint (x,y) depends on it's distance from the line. #glm


The equation for the probability is the same as yesterday but with the difference between our model line and the datapoint. #glm


And then again flip to likelihood. We've seen this data point, what is the likelihood of our parameter β? #glm

Again same as yesterday, when we see many data points (xi, yi), we multiply all the likelihoods together (with Π). #glm



And the maximum likelihood estimate for β is just the value for which this function is biggest. #glm




Now some conceptually unimportant, but useful in practice comments about how we find the highest value of a likelihood function. #glm

An easy computational way to do it is just calculate the likelihood for loads of values of your parameter (eg β) and take the biggest. #glm

Smarter ways to find the maximum require extra maths. Our likelihood function is difficult to deal with because of the product. #glm

So we look at the log likelihood. Like transforming data, if we find the maximum log likelihood then untranform it, we get the right result. #glm

log(AB) = logA + logB. So logging the likelihood turns the product into a sum, and gets rid of the e^stuff, making things easier. #glm


log(e^A) = A, so the log likelihood also remove the exponential, making things easier still. #glm

To find the maximum of a function we differentiate it and solve that equation. This possible for our lm likelihood. #glm http://people.su.se/~palme/mllinear.pdf

## Max likelihood Poisson LM



While it might seem easier to look at link functions as the next step in this #glm series, I think looking at a different family is better.

So today I'll look at a poisson #glm ie glm(y~x, family=poisson(link=identity)). The identity link just means we fit a straight line still.

A quick aside. The poisson distribution is discrete but we are basically going to ignore that. #glm http://stats.stackexchange.com/questions/38530/how-does-a-poisson-distribution-work-when-modeling-continuous-data-and-does-it-r


So the approach is basically the same as the normal error lm. We have a linear model but with poisson errors. #glm

Again we can assume a fixed gradient β and work out the probability of a point (x,y) before flipping to likelihood. #glm

There's a difference in that we don't look at the deviance from our line y-xβ, but always compare from 0. #glm

And then we flip to likelihood. What's the probability of β, given the datapoints we have seen. #glm

Again switching to log likelihood makes the eqn a bit easier. But it still has to be solved numerically. #glm


The last part of the #glm, that doesn't require much discussion is the link function.




## Link function
